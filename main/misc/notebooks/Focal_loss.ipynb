{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eece45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "c5f82243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_focal_loss_with_logits(truth, logits,\n",
    "                                  gamma = 0, alpha = None):\n",
    "    \"\"\"\n",
    "    Implementation of binary focal loss function \n",
    "    \n",
    "    truth:\n",
    "        Ground truth confidence, i.e. 1 for close anchors, 0 for anchors\n",
    "        that are too far off and -1 for anchors to be ignored. Must have\n",
    "        shape (?, fh, fw, k)\n",
    "        \n",
    "    logits:\n",
    "        PPN confidence output, must have shape (?, fh, fw, k).\n",
    "        \n",
    "    gamma: \n",
    "        As defined in mathematical equation\n",
    "    \n",
    "    alpha:\n",
    "        As defined in mathematical equation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using sigmoid not softmax as is typical for binary classifiers\n",
    "    x = nn.functional.sigmoid(logits)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "73593c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryFocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int,long)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "            \n",
    "    def forward():\n",
    "        return\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "8089a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_loss(reg_logits,reg_gt,conf_gt):\n",
    "    \"\"\"\n",
    "    Function to calculate the regression part of \n",
    "    loss function. Masks all elements of ground \n",
    "    truth with confidence not equal to 1 then \n",
    "    calculates mean squared error.\n",
    "        \n",
    "    reg_logits: \n",
    "        Predicted normalised regression for each\n",
    "        origin (?, fh, fw, 2)\n",
    "    \n",
    "    reg_gt: \n",
    "        Regression truth for each origin (?, fh, fw, 2)\n",
    "    \n",
    "    conf_gt:\n",
    "        Confidence ground truth for each origin\n",
    "        (0 for no galaxy close enough or 1 for \n",
    "        galaxy close enough) (?, fh, fw, 1)\n",
    "    \"\"\"\n",
    "    regLoss = nn.MSELoss(reduction=\"mean\")\n",
    "    pos_mask = torch.eq(conf_gt,1)\n",
    "    \n",
    "    batch_size = conf_gt.shape[0]\n",
    "    dim = conf_gt.shape[2]\n",
    "\n",
    "    return regLoss(reg_logits[pos_mask.expand(batch_size,dim,dim,2)],\n",
    "            reg_gt[pos_mask.expand(batch_size,dim,dim,2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a0955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing reg_loss\n",
    "dim_ = 2\n",
    "batch_ = 3\n",
    "x = torch.rand(batch,dim,dim,2)\n",
    "y = torch.rand(batch,dim,dim,2)\n",
    "conf_gt = torch.randint(low=0,high=2,size=(batch,dim,dim,1))\n",
    "pos_mask_2 = torch.eq(conf_gt,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952fd47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regLoss(x[pos_mask_2.expand(batch,dim,dim,2)],y[pos_mask_2.expand(batch,dim,dim,2)]))\n",
    "print(reg_loss(y,x,conf_gt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "25e6cc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = {\n",
    "    \"image_size\": 224,\n",
    "    \"feature_size\": 56,\n",
    "    \"r_far\": np.sqrt(0.5*0.5 + 0.5*0.5),\n",
    "    \"r_near\": np.sqrt(0.5*0.5 + 0.5*0.5),\n",
    "    \"N_conf\": 1/128.0,\n",
    "    \"N_reg\": 1/128.0,\n",
    "    \"Conf_loss_fn\": 'crossentropy'\n",
    "}\n",
    "\n",
    "# N_reg = N_conf = 1/batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "aa226f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(reg_logits,reg_gt,conf_logits,conf_gt,config):\n",
    "    \"\"\"\n",
    "    Function to calculate the total loss function.\n",
    "    Combines the regressional loss and confidence loss.\n",
    "    \n",
    "    reg_gt: \n",
    "        Regression truth for each origin (?, fh, fw, 2)\n",
    "        \n",
    "    reg_logits: \n",
    "        Predicted normalised regression for each\n",
    "        origin (?, fh, fw, 2)\n",
    "    \n",
    "    conf_gt:\n",
    "        Confidence ground truth for each origin\n",
    "        (0 for no galaxy close enough or 1 for \n",
    "        galaxy close enough) (?, fh, fw, 1)\n",
    "    \n",
    "    conf_logits:\n",
    "        Confidence prediction for each origin. Note \n",
    "        this is before sigmoid applied. (?, fh, fw, 1)\n",
    "        \n",
    "    config:\n",
    "        Config file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate regression part of loss function\n",
    "    regression_loss = reg_loss(reg_logits,reg_gt,conf_gt)\n",
    "    \n",
    "    # Define which points to include in confidence mask\n",
    "    conf_mask = torch.ne(conf_gt,-1)\n",
    "    \n",
    "    if config['Conf_loss_fn'] == 'CrossEntropy':\n",
    "        CrossEntropy = nn.CrossEntropyLoss(weight=None, \n",
    "                        size_average=None, ignore_index=- 1, \n",
    "                        reduce=None, reduction='mean',\n",
    "                        label_smoothing=0.0\n",
    "                        )\n",
    "        confidence_loss = CrossEntropy(conf_logits[conf_mask],\n",
    "                                       conf_gt[conf_mask])\n",
    "        \n",
    "    else:\n",
    "        return \"Need to implement focal loss if necessary\"\n",
    "    \n",
    "    N_conf, N_reg = config['N_conf'], config['N_reg']\n",
    "    \n",
    "    return (N_conf * confidence_loss + N_reg * regression_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "3f0fceeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8133)\n",
      "tensor(0.0990)\n",
      "tensor(0.8133)\n",
      "tensor(0.9122)\n"
     ]
    }
   ],
   "source": [
    "# Testing reg_loss\n",
    "dim_ = 2\n",
    "batch_ = 3\n",
    "x = torch.rand(batch,dim,dim,2)\n",
    "y = torch.rand(batch,dim,dim,2)\n",
    "conf_gt = torch.randint(low=0,high=2,size=(batch,dim,dim,1),dtype=torch.float32)\n",
    "conf_logits = torch.randint(low=0,high=2,size=(batch,dim,dim,1),dtype=torch.float32)\n",
    "pos_mask_2 = torch.eq(conf_gt,1)\n",
    "\n",
    "CE = nn.CrossEntropyLoss(weight=None, \n",
    "                    size_average=None, ignore_index=- 1, \n",
    "                    reduce=None, reduction='mean',\n",
    "                    label_smoothing=0.0)\n",
    "\n",
    "print(CE(conf_logits,conf_gt))\n",
    "print(total_loss(x,y,conf_logits,conf_gt, test_config)*128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88122e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## FULL MODEL FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b60aeb24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ True  True  True  True  True  True  True]\n",
      "   [ True  True  True  True  True  True  True]\n",
      "   [ True  True  True  True  True  True  True]\n",
      "   [ True  True  True  True  True  True  True]\n",
      "   [ True  True  True  True  True  True  True]\n",
      "   [ True  True  True  True  True  True  True]\n",
      "   [ True  True  True  True  True  True  True]]]\n",
      "\n",
      "\n",
      " [[[ True  True  True  True  True  True  True]\n",
      "   [ True  True  True  True  True  True  True]\n",
      "   [ True  True  True  True  True  True  True]\n",
      "   [ True  True  True  True  True  True  True]\n",
      "   [ True  True  True  True  True  True  True]\n",
      "   [ True  True  True  True  True  True  True]\n",
      "   [ True  True  True  True  True  True  True]]]]\n",
      "tensor(0., dtype=torch.float64)\n",
      "tensor([[[[True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True]]],\n",
      "\n",
      "\n",
      "        [[[True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True],\n",
      "          [True, True, True, True, True, True, True]]]])\n",
      "tensor(0., dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "test_config = {\n",
    "    \"image_size\": 224,\n",
    "    \"feature_size\": 7,\n",
    "    \"r_far\": np.sqrt(0.5*0.5 + 0.5*0.5),\n",
    "    \"r_near\": np.sqrt(0.5*0.5 + 0.5*0.5),\n",
    "    \"N_conf\": 1/1.0,\n",
    "    \"N_reg\": 1/1.0,\n",
    "    \"Conf_loss_fn\": 'CrossEntropy',\n",
    "    \"layers\": [8,4,2,1],\n",
    "    \"batch_size\": 1\n",
    "}\n",
    "\n",
    "def reg_loss(reg_logits,reg_gt,conf_gt):\n",
    "    \"\"\"\n",
    "    Function to calculate the regression part of \n",
    "    loss function. Masks all elements of ground \n",
    "    truth with confidence not equal to 1 then \n",
    "    calculates mean squared error.\n",
    "        \n",
    "    reg_logits: \n",
    "        Predicted normalised regression for each\n",
    "        origin (?, 2, fh, fw)\n",
    "        \n",
    "    reg_gt: \n",
    "        Regression truth for each origin (?, 2, fh, fw)\n",
    "    \n",
    "    conf_gt:\n",
    "        Confidence ground truth for each origin\n",
    "        (0 for no galaxy close enough or 1 for \n",
    "        galaxy close enough) (?, 1, fh, fw)\n",
    "    \"\"\"\n",
    "    regLoss = nn.MSELoss(reduction=\"mean\")\n",
    "    pos_mask = torch.eq(conf_gt.expand(-1,2,-1,-1),1)\n",
    "    #print(pos_mask)\n",
    "    #print(reg_logits[pos_mask] == reg_gt[pos_mask])\n",
    "    #print(reg_gt[pos_mask])\n",
    "    return regLoss(reg_logits[pos_mask],reg_gt[pos_mask])\n",
    "\n",
    "\n",
    "def total_loss(reg_logits,reg_gt,conf_logits,conf_gt,config):\n",
    "    \"\"\"\n",
    "    Function to calculate the total loss function.\n",
    "    \n",
    "    Combines the regressional loss and confidence loss.\n",
    "    \n",
    "    Masks all confidence values equal to -1 then calculates\n",
    "    confidence loss with BinaryCrossEntropy with logits. \n",
    "    \n",
    "    If needed can add focal loss however will see how \n",
    "    training goes to start with.\n",
    "    \n",
    "    reg_gt: \n",
    "        Regression truth for each origin (?, 2, fh, fw)\n",
    "        \n",
    "    reg_logits: \n",
    "        Predicted normalised regression for each\n",
    "        origin (?, 2, fh, fw)\n",
    "    \n",
    "    conf_gt:\n",
    "        Confidence ground truth for each origin\n",
    "        (0 for no galaxy close enough or 1 for \n",
    "        galaxy close enough) (?, 1, fh, fw)\n",
    "    \n",
    "    conf_logits:\n",
    "        Confidence prediction for each origin. Note \n",
    "        this is before sigmoid applied. (?, 1, fh, fw)\n",
    "        \n",
    "    config:\n",
    "        Config file\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate regression part of loss function\n",
    "    regression_loss = reg_loss(reg_logits,reg_gt,conf_gt)\n",
    "    print(regression_loss)\n",
    "    \n",
    "    # Define which points to include in confidence mask\n",
    "    conf_mask = torch.ne(conf_gt,-1)\n",
    "    print(conf_mask)\n",
    "    \n",
    "    if config['Conf_loss_fn'] == 'CrossEntropy':\n",
    "        \n",
    "        #CrossEntropy = nn.CrossEntropyLoss(weight=None, \n",
    "        #                size_average=None, ignore_index=- 1, \n",
    "        #                reduce=None, reduction='mean',\n",
    "        #                label_smoothing=0.0\n",
    "        #                )\n",
    "        #confidence_loss = CrossEntropy(conf_logits[conf_mask],\n",
    "        #                               conf_gt[conf_mask])\n",
    "        \n",
    "        # need to use withlogitsloss in actually model as haven't sigmoided yet\n",
    "        # but the ground truth has been sigmoided so need to just use BCELoss\n",
    "        #WithLogitsLoss\n",
    "        BCE = torch.nn.BCELoss(weight=None, size_average=None, \n",
    "                                         reduce=None, reduction='mean',\n",
    "                                         )\n",
    "         \n",
    "        confidence_loss = BCE(conf_logits[conf_mask],conf_gt[conf_mask])\n",
    "        \n",
    "    else:\n",
    "        return \"Need to implement focal loss if necessary\"\n",
    "    \n",
    "    print(confidence_loss)\n",
    "    \n",
    "    N_conf, N_reg = config['N_conf'], config['N_reg']\n",
    "    \n",
    "    return (N_conf * confidence_loss + N_reg * regression_loss)\n",
    "\n",
    "\n",
    "\n",
    "dir_path = \"/Users/edroberts/Desktop/im_gen/training_data/train\"\n",
    "os.chdir(dir_path)\n",
    "\n",
    "images = np.load('training_images.npy')\n",
    "test_im[0] = images[0]\n",
    "test_im = np.zeros((1,1,224,224))\n",
    "test_conf = np.zeros((2,1,7,7))\n",
    "test_reg = np.zeros((2,2,7,7))\n",
    "anchors  = np.load(\"anchors.npy\")\n",
    "\n",
    "os.chdir(dir_path+\"/anchor_labels\")\n",
    "test_conf[0] = np.load(\"test_0000_conf.npy\")\n",
    "test_reg[0] = np.load(\"test_0000_reg.npy\")\n",
    "test_conf[1] = np.load(\"test_0001_conf.npy\")\n",
    "test_reg[1] = np.load(\"test_0001_reg.npy\")\n",
    "\n",
    "print(test_conf == test_conf)\n",
    "ttest_conf = torch.tensor(test_conf)\n",
    "ttest_reg = torch.tensor(test_reg)\n",
    "#print(test_conf.shape)\n",
    "#print(test_reg.shape)\n",
    "\n",
    "#edited_ttest_conf = torch.clone(ttest_conf)\n",
    "#print(edited_ttest_conf)\n",
    "#edited_ttest_conf[0,0,0,0] = 1\n",
    "#print(edited_ttest_conf)\n",
    "\n",
    "#edited_ttest_reg = torch.clone(ttest_reg)\n",
    "##print(edited_ttest_reg)\n",
    "#edited_ttest_reg[0,0,0,1] = -1.3125\n",
    "#print(edited_ttest_reg)\n",
    "\n",
    "#print(reg_loss(edited_ttest_reg,ttest_reg,ttest_conf))\n",
    "\n",
    "print(total_loss(ttest_reg,ttest_reg,\n",
    "                 ttest_conf,ttest_conf,\n",
    "                 test_config))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
