{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24f7e1cf",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "\n",
    "This script does all the image pre-processing: opens .fits files and selects appropriate layer, standardises the pixel values to have mean 0 and std 1, opens files with centre values in, calculates each image's anchors labels. Saves dataset in the .hdf5 format. (Also functions to write data in .npy format, either as seperate files for each image or files stack with all images, these functions are old and not called)\n",
    "\n",
    "All this pre-processing saves time in training the open because these operations only have to be performed once here, and hence aren't repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72319511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e048e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_fits(start,num,path):\n",
    "    \"\"\"\n",
    "    Functions to add extra padding to file names. Essentially I decided I \n",
    "    needed more than 9999 files but had already generated all these so \n",
    "    instead of re-runing their generation I decided to simply pad a\n",
    "    0 to their values\n",
    "    \"\"\"\n",
    "    os.chdir(path)\n",
    "    for i in range(start,num):\n",
    "        if os.path.isfile(('test_'+f'{i:04}'+'.fits')):\n",
    "            os.rename(('test_'+f'{i:04}'+'.fits'),('test_'+f'{i:05}'+'.fits'))\n",
    "    return \n",
    "\n",
    "def pad_csv(start,num,path):\n",
    "    \"\"\"\n",
    "    Functions to add extra padding. Essentially I decided I needed\n",
    "    more than 9999 files but had already generated all these so \n",
    "    instead of re-runing their generation I decided to simply pad a\n",
    "    0 to their values\n",
    "    \"\"\"\n",
    "    os.chdir(path)\n",
    "    \n",
    "    for i in range(start,num):\n",
    "        if os.path.isfile(('test_'+f'{i:04}'+'.csv')):\n",
    "            print(i)\n",
    "            os.rename(('test_'+f'{i:04}'+'.csv'),('test_'+f'{i:05}'+'.csv'))\n",
    "    return\n",
    "\n",
    "#pad_fits(0,9999,\"/Users/edroberts/Desktop/im_gen/training_data/train/fits\")\n",
    "#pad_csv(0,9999,\"/Users/edroberts/Desktop/im_gen/training_data/train/csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7312c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_files(expected_number, path, name_type):\n",
    "    \"\"\"\n",
    "    Function to make a list of all files that exist.\n",
    "    \n",
    "    Gets around missing data in dataset instead of painfully\n",
    "    going through and regenerating.\n",
    "    \n",
    "    The bug in the generation python script appears to be when\n",
    "    the max/min number of galaxies are reached.\n",
    "    \n",
    "    It is quicker to simply ignore missing data than sorting \n",
    "    out the perculiar bug. The total number of images doesn't \n",
    "    necessarily need to be any number, however should be split in \n",
    "    batches which are a power of 2.\n",
    "    \"\"\"\n",
    "    actual_file_numbers = []\n",
    "    missing_file_numbers = []\n",
    "    os.chdir(path)\n",
    "    \n",
    "    count = 0\n",
    "    for i in range(expected_number):\n",
    "        if os.path.isfile((name_type+'_'+f'{i:05}'+'.fits')):\n",
    "            count += 1\n",
    "            actual_file_numbers.append(f'{i:05}')\n",
    "        else:\n",
    "            missing_file_numbers.append(f'{i:05}')\n",
    "    \n",
    "    return actual_file_numbers, missing_file_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d0cfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_fits(file_list_numbers, pixel_x, pixel_y,path, name_type):\n",
    "    \"\"\"\n",
    "    Function to open .fits files and select image layer\n",
    "    \"\"\"\n",
    "    data_images = np.zeros((len(file_list_numbers),1,pixel_y,pixel_x))\n",
    "\n",
    "    #moving to right directory\n",
    "    os.chdir(path)\n",
    "    \n",
    "    print(\"OPENING FITS...\")\n",
    "    \n",
    "    # Opening an selecting appropriate .fits layer\n",
    "    for count, i in enumerate(file_list_numbers):\n",
    "        #print(\"Count: \"+str(count)+\" file num: \" +str(i))\n",
    "        with fits.open(name_type+'_'+i+'.fits') as hdul:\n",
    "            #hdul.info()\n",
    "            data_images[count] = np.array(hdul[1].data);\n",
    "            \n",
    "    print(\"SUCCESSFULLY OPENED FITS \\n\")\n",
    "    \n",
    "    return data_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "012b3aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_images(data_images,pixel_y,pixel_x):\n",
    "    \"\"\"\n",
    "    Function to rescale the pixel values of \n",
    "    an image to have mean 0 and standard\n",
    "    deviation 1\n",
    "    \"\"\"\n",
    "    \n",
    "    img_std = np.zeros((len(data_images),1,pixel_y,pixel_x))\n",
    "    \n",
    "    print(\"STANDARDISING PIXEL VALUES...\")\n",
    "    \n",
    "    for i in tqdm(range(len(data_images))):\n",
    "        mean = np.mean(data_image[i])\n",
    "        std = np.std(data_image[i])\n",
    "        img_std[i] = (data_image[i]-mean)/std\n",
    "        \n",
    "    print(\"STANDARDISATION COMPLETE \\n\")\n",
    "        \n",
    "    return img_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fb617cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anchors(config):\n",
    "    \"\"\"\n",
    "    Generates a lattice of anchors for a given image input\n",
    "    size and feature map size. Note the coordinates are \n",
    "    (x,y) indexed!\n",
    "    \n",
    "    Adapted from Duncan Tilley's work on PPN \n",
    "    https://github.com/tilleyd/point-proposal-net/\n",
    "    \n",
    "    config\n",
    "        The configuration dictionary.\n",
    "\n",
    "    Returns the coorinates of the origin anchors (x,y).\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Reads sizes\n",
    "    img_size = config['image_size']\n",
    "    feature_size = config['feature_size']\n",
    "    \n",
    "    # Calculates step length for given sizes\n",
    "      d i  s p l  a  y                 \n",
    "    halfstep = step * 0.5\n",
    "    \n",
    "    # Calculates the lattice of points\n",
    "    x = np.arange(halfstep, img_size, step, dtype=np.float32)\n",
    "    y = np.arange(halfstep, img_size, step, dtype=np.float32)\n",
    "    \n",
    "    return np.transpose([np.tile(x, len(y)), np.repeat(y, len(x))])\n",
    "\n",
    "\n",
    "def get_anchor_labels(anchors, truth_coords, config):\n",
    "    \"\"\"\n",
    "    Creates anchor labels to be used in training.\n",
    "    \n",
    "    Adapted from Duncan Tilley's work on PPN \n",
    "    https://github.com/tilleyd/point-proposal-net/\n",
    "    \n",
    "    anchors\n",
    "        The list of anchor coordinates generated from \n",
    "        get_anchors()\n",
    "    \n",
    "    truth_coords\n",
    "        The list of ground truth point cooridnates.\n",
    "        Using (x,y) convention.\n",
    "    \n",
    "    config\n",
    "        The configuration dictionary.\n",
    "    \n",
    "    Returns y_conf, y_reg with shape (7,7), (2,7,7) \n",
    "    \"\"\"\n",
    "\n",
    "    r_near = config['r_near']\n",
    "    r_far = config['r_far']\n",
    "    img_size = config['image_size']\n",
    "    feature_size = config['feature_size']\n",
    "    \n",
    "    step = img_size / feature_size\n",
    "    halfstep = step * 0.5\n",
    "\n",
    "    # Initialising output\n",
    "    y_conf = np.full(anchors.shape[0], -1, dtype=np.int8)\n",
    "    y_reg = np.zeros(anchors.shape)\n",
    "    \n",
    "    # For each true point, find the nearest anchor\n",
    "    for (x, y) in truth_coords:\n",
    "        \n",
    "        # Normalising truth points to step size\n",
    "        x_norm = (x - halfstep) / step\n",
    "        y_norm = (y - halfstep) / step\n",
    "        \n",
    "        # Finding index of closest anchor by rounding\n",
    "        r = int(np.round(y_norm))\n",
    "        c = int(np.round(x_norm))   \n",
    "        anchor_index = r * feature_size + c\n",
    "        \n",
    "        # Setting values for anchor index\n",
    "        y_conf[anchor_index] = 1\n",
    "        y_reg[anchor_index][0] = (x - anchors[anchor_index][0]) / step\n",
    "        y_reg[anchor_index][1] = (y - anchors[anchor_index][1]) / step\n",
    "    \n",
    "    # For each anchor calculate the distance to each point\n",
    "    for i in range(len(anchors)):\n",
    "        x, y = anchors[i]\n",
    "        x /= step\n",
    "        y /= step\n",
    "        distances = []\n",
    "        for (px, py) in truth_coords:\n",
    "            px /= step\n",
    "            py /= step\n",
    "            distances.append(np.sqrt((x-px)**2 + (y-py)**2))\n",
    "        \n",
    "        if len(distances) > 0:\n",
    "            near = np.argmin(distances)\n",
    "            dist = distances[near]\n",
    "            \n",
    "            if dist <= r_near:\n",
    "                y_conf[i] = 1\n",
    "                px, py = truth_coords[near]\n",
    "                \n",
    "                px /= step\n",
    "                py /= step\n",
    "                \n",
    "                y_reg[i][0] = (px - x)\n",
    "                y_reg[i][1] = (py - y)\n",
    "                \n",
    "            elif dist > r_far:\n",
    "                y_conf[i] = 0\n",
    "                \n",
    "    # reshape for use in PPN training\n",
    "    y_conf = np.reshape(y_conf, (feature_size, feature_size))\n",
    "    y_reg = np.reshape(y_reg, (feature_size, feature_size) + (2,))\n",
    "    y_reg = np.transpose(y_reg, (2, 0, 1))\n",
    "    \n",
    "    return y_conf, y_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d5a2b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_and_label_centres(path_main, path_csv, conf_array, reg_array, file_list, config):\n",
    "    \"\"\"\n",
    "    Function to open centres .csv files for each image and then \n",
    "    pass centre coordinates to get_anchor_labels so that we get\n",
    "    ground truth labesl for each image.\n",
    "    \n",
    "    Also calculates and saves anchors\n",
    "    \"\"\"\n",
    "    # Note ordering only works up to 99999 as numbers on file names padded to 00000\n",
    "    csvdir_path = pathlib.Path(path_csv)\n",
    "    csv_file_list = sorted([str(path) for path in csvdir_path.glob('*.csv')])\n",
    "    \n",
    "    \n",
    "    # Define origin anchor positions and save to \"anchors.npy\" file\n",
    "    anchors = get_anchors(config)\n",
    "    os.chdir(path_main)\n",
    "    np.save(\"anchors\",anchors)\n",
    "    \n",
    "    print(\"CALCULATING LABELS... \")\n",
    "    \n",
    "    # For each image, read the file with centre locations \n",
    "    for i in tqdm(range(len(file_list))):\n",
    "        df_center = pd.read_csv(csv_file_list[i], sep=',', header=None)\n",
    "        np_center = df_center.values\n",
    "        np_center[[0,1]] = np_center[[1,0]] # Swapping from y,x to x,y\n",
    "        \n",
    "        # Remember .T for np_centers file to transpose from collumns to rows\n",
    "        conf_labels[i], reg_labels[i] = get_anchor_labels(anchors, np_center.T, config)\n",
    "\n",
    "    print(\"LABELS DONE \\n\")\n",
    "    \n",
    "    return anchors, conf_array, reg_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fe80054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_HDF5(path, name, image_array, conf_array, reg_array):\n",
    "    \"\"\"\n",
    "    Function to write the dataset to HDF5 format\n",
    "    \"\"\"\n",
    "    if len(image_array) != len(conf_labels):\n",
    "        return \"ERROR length image_array != conf_labels\"\n",
    "    \n",
    "    if len(image_array) != len(reg_labels):\n",
    "        return \"ERROR length image_array != reg_labels\"\n",
    "    \n",
    "    if len(conf_array) != len(reg_labels):\n",
    "        return \"ERROR length conf_array != reg_labels\"\n",
    "    \n",
    "    print(\"WRITING HDF5 FILE...\")\n",
    "    \n",
    "    os.chdir(path)\n",
    "    with h5py.File(name+'.hdf5', 'w') as f:\n",
    "        dset_images = f.create_dataset(\"images\", data=image_array)\n",
    "        dset_conf = f.create_dataset(\"confidence\", data=conf_array)\n",
    "        dset_reg = f.create_dataset(\"regression\", data=reg_array)\n",
    "        \n",
    "    print(\"HDF5 datafile written\")\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b70d9058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_single_images(image_array, file_names ,base_path):\n",
    "    \"\"\"\n",
    "    !!!Retired function: now using hdf5!!!\n",
    "    \n",
    "    Function to save images as individual files\n",
    "    \"\"\"\n",
    "    \n",
    "    os.chdir(base_path)\n",
    "    \n",
    "    if not os.path.isfile(('/images')):\n",
    "        os.mkdir(base_path+\"/images\");\n",
    "    os.chdir(base_path+\"/images\");\n",
    "    \n",
    "    for i in range(len(image_array)):\n",
    "        np.save(file_names[i]+\"_im\", image_array[i])\n",
    "        \n",
    "        \n",
    "def save_stack_images(image_array, base_path, name):\n",
    "    \"\"\"\n",
    "    !!!Retired function: now using hdf5!!!\n",
    "    \n",
    "    Save images all in one stacked npy file\n",
    "    \"\"\"\n",
    "    os.chdir(base_path)\n",
    "    np.save(name,image_array)\n",
    "    \n",
    "    \n",
    "def write_label_npy(path, main_name, conf_array, reg_array):\n",
    "    \"\"\"\n",
    "    !!!Retired function: now using hdf5!!!\n",
    "    \n",
    "    Function to write data as npy files.\n",
    "    \n",
    "    Each label data is stored in sepearte npy file\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(len(conf_array))):\n",
    "        if i == 0:\n",
    "            os.mkdir(dir_path+\"/anchor_labels\");\n",
    "        os.chdir(dir_path+\"/anchor_labels\");\n",
    "        \n",
    "        np.save(main_name+\"_conf\",conf_array[i])\n",
    "        np.save(main_name+\"_reg\",reg_array[i])\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4def3fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = {\n",
    "    \"image_size\": 224,\n",
    "    \"feature_size\": 7,\n",
    "    \"r_far\": np.sqrt(0.5*0.5 + 0.5*0.5),\n",
    "    \"r_near\": np.sqrt(0.5*0.5 + 0.5*0.5),\n",
    "    \"batch_size\": 128\n",
    "    \n",
    "}\n",
    "\n",
    "prepros_config = {\n",
    "    \"path_to_fits\": '/Users/edroberts/Desktop/im_gen/training_data/train/fits',\n",
    "    \"path_to_csv\": '/Users/edroberts/Desktop/im_gen/training_data/train/csv',\n",
    "    \"path_to_main\": '/Users/edroberts/Desktop/im_gen/training_data/train',\n",
    "    \"image_name\": 'train',\n",
    "    \"hdf5_name\": 'dataset',\n",
    "    \"expected_img_number\": 30093\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "043c319e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images in the dataset is: 29824\n",
      "\n",
      "OPENING FITS...\n",
      "SUCCESSFULLY OPENED FITS \n",
      "\n",
      "STANDARDISING PIXEL VALUES...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 29824/29824 [01:30<00:00, 330.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STANDARDISATION COMPLETE \n",
      "\n",
      "CALCULATING LABELS... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 29824/29824 [00:49<00:00, 602.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABELS DONE \n",
      "\n",
      "WRITING HDF5 FILE...\n",
      "HDF5 datafile written\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "files , missing_files = find_files(prepros_config[\"expected_img_number\"], \n",
    "                                   prepros_config['path_to_fits'], \n",
    "                                   prepros_config['image_name'])\n",
    "\n",
    "print(\"Total number of images in the dataset is: \" + str(len(files)) + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "data_image = open_fits(files,\n",
    "                       test_config['image_size'],\n",
    "                       test_config['image_size'],\n",
    "                       prepros_config['path_to_fits'],\n",
    "                       prepros_config['image_name'])\n",
    "\n",
    "std_image = standardise_images(data_image,\n",
    "                               test_config['image_size'],\n",
    "                               test_config['image_size'])\n",
    "\n",
    "\n",
    "\n",
    "conf_labels = np.full((len(files), 1, test_config['feature_size'],\n",
    "                       test_config['feature_size']), -1, dtype=np.float64)\n",
    "\n",
    "reg_labels = np.full((len(files), 2, test_config['feature_size'],\n",
    "                    test_config['feature_size']), -1, dtype=np.float64)\n",
    "\n",
    "\n",
    "anchors, conf_labels, reg_labels = open_and_label_centres(prepros_config['path_to_main'], \n",
    "                                                          prepros_config['path_to_csv'],\n",
    "                                                          conf_labels, reg_labels, files, \n",
    "                                                          test_config)\n",
    "\n",
    "\n",
    "\n",
    "write_HDF5(prepros_config['path_to_main'], prepros_config['hdf5_name'], std_image, conf_labels, reg_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0e4ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Testing Code \n",
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a322baf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 1. 0. 1. 0. 1.]\n",
      "  [0. 1. 1. 1. 1. 1. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 0. 1. 0. 0.]\n",
      "  [1. 1. 1. 0. 1. 1. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(conf_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68815eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.       0.      -0.34375  0.      -0.21875  0.      -0.09375]\n",
      "  [ 0.      -0.0625   0.34375 -0.65625 -0.21875 -0.03125 -0.3125 ]\n",
      "  [ 0.       0.       0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.       0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.       0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.4375  -0.5625   0.      -0.3125   0.       0.     ]\n",
      "  [ 0.3125  -0.6875   0.25     0.       0.375   -0.625    0.     ]]\n",
      "\n",
      " [[ 0.       0.       0.3125   0.       0.53125  0.      -0.09375]\n",
      "  [ 0.       0.03125 -0.0625  -0.0625  -0.46875 -0.21875 -0.15625]\n",
      "  [ 0.       0.       0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.       0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.       0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.125    0.125    0.      -0.21875  0.       0.     ]\n",
      "  [ 0.09375  0.09375 -0.21875  0.      -0.125   -0.125    0.     ]]]\n"
     ]
    }
   ],
   "source": [
    "print(reg_labels[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcf2a56e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-7.43697702e-01  7.47213505e-02  5.80275532e-04 ... -4.21049533e-01\n",
      "   -2.64094733e-01 -9.04092544e-01]\n",
      "  [ 1.60494417e-01 -3.37417740e-01 -1.52784548e-01 ... -2.02557245e-02\n",
      "    7.10716564e-03 -1.09733670e-01]\n",
      "  [-1.37637574e-01 -3.08360830e-01 -1.66486455e-01 ... -7.33495881e-03\n",
      "   -1.38798094e-01 -4.21471036e-01]\n",
      "  ...\n",
      "  [-9.00132065e-02 -3.78478240e-01  7.20108293e-02 ... -9.17051004e-01\n",
      "   -6.21540302e-01 -7.25207750e-01]\n",
      "  [-1.16144576e-01 -6.30424141e-01 -7.34811653e-01 ... -4.59396359e-02\n",
      "   -3.35438794e-01  7.99345842e-02]\n",
      "  [-7.43160374e-01 -1.95464234e-01 -6.97024824e-01 ... -1.36553410e-01\n",
      "   -7.52544821e-02 -3.83026646e-01]]]\n",
      "[[[0. 0. 1. 0. 1. 0. 1.]\n",
      "  [0. 1. 1. 1. 1. 1. 1.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 1. 1. 0. 1. 0. 0.]\n",
      "  [1. 1. 1. 0. 1. 1. 0.]]]\n",
      "[[[ 0.       0.      -0.34375  0.      -0.21875  0.      -0.09375]\n",
      "  [ 0.      -0.0625   0.34375 -0.65625 -0.21875 -0.03125 -0.3125 ]\n",
      "  [ 0.       0.       0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.       0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.       0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.4375  -0.5625   0.      -0.3125   0.       0.     ]\n",
      "  [ 0.3125  -0.6875   0.25     0.       0.375   -0.625    0.     ]]\n",
      "\n",
      " [[ 0.       0.       0.3125   0.       0.53125  0.      -0.09375]\n",
      "  [ 0.       0.03125 -0.0625  -0.0625  -0.46875 -0.21875 -0.15625]\n",
      "  [ 0.       0.       0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.       0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.       0.       0.       0.       0.       0.     ]\n",
      "  [ 0.       0.125    0.125    0.      -0.21875  0.       0.     ]\n",
      "  [ 0.09375  0.09375 -0.21875  0.      -0.125   -0.125    0.     ]]]\n"
     ]
    }
   ],
   "source": [
    "# Testing to see if .hdf5 written correctly\n",
    "x=1\n",
    "os.chdir('/Users/edroberts/Desktop/im_gen/training_data/train')\n",
    "with h5py.File('dataset.hdf5', 'r') as f:\n",
    "    image = f['images'][x]\n",
    "    conf = f['confidence'][x]\n",
    "    reg = f['regression'][x]\n",
    "    \n",
    "print(image)\n",
    "print(conf)\n",
    "print(reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c4e3797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19821\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/Users/edroberts/Desktop/im_gen/training_data/train')\n",
    "with h5py.File('dataset.hdf5', 'r') as f:\n",
    "    conf_shape = f['confidence'].shape\n",
    "    \n",
    "print(conf_shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2f300356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x12d789eb0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(conf_labels[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73830c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "basename = os.path.splitext(os.path.basename(csv_file_list[i]))[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
